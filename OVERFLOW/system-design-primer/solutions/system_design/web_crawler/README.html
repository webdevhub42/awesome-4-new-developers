<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<h1 id="design-a-web-crawler">Design a web crawler</h1>
<p><em>Note: This document links directly to relevant areas found in the <a href="https://github.com/donnemartin/system-design-primer#index-of-system-design-topics">system design topics</a> to avoid duplication. Refer to the linked content for general talking points, tradeoffs, and alternatives.</em></p>
<h2 id="step-1-outline-use-cases-and-constraints">Step 1: Outline use cases and constraints</h2>
<blockquote>
<p>Gather requirements and scope the problem. Ask questions to clarify use cases and constraints. Discuss assumptions.</p>
</blockquote>
<p>Without an interviewer to address clarifying questions, we’ll define some use cases and constraints.</p>
<h3 id="use-cases">Use cases</h3>
<h4 id="well-scope-the-problem-to-handle-only-the-following-use-cases">We’ll scope the problem to handle only the following use cases</h4>
<ul>
<li><strong>Service</strong> crawls a list of urls:
<ul>
<li>Generates reverse index of words to pages containing the search terms</li>
<li>Generates titles and snippets for pages
<ul>
<li>Title and snippets are static, they do not change based on search query</li>
</ul></li>
</ul></li>
<li><strong>User</strong> inputs a search term and sees a list of relevant pages with titles and snippets the crawler generated
<ul>
<li>Only sketch high level components and interactions for this use case, no need to go into depth</li>
</ul></li>
<li><strong>Service</strong> has high availability</li>
</ul>
<h4 id="out-of-scope">Out of scope</h4>
<ul>
<li>Search analytics</li>
<li>Personalized search results</li>
<li>Page rank</li>
</ul>
<h3 id="constraints-and-assumptions">Constraints and assumptions</h3>
<h4 id="state-assumptions">State assumptions</h4>
<ul>
<li>Traffic is not evenly distributed
<ul>
<li>Some searches are very popular, while others are only executed once</li>
</ul></li>
<li>Support only anonymous users</li>
<li>Generating search results should be fast</li>
<li>The web crawler should not get stuck in an infinite loop
<ul>
<li>We get stuck in an infinite loop if the graph contains a cycle</li>
</ul></li>
<li>1 billion links to crawl
<ul>
<li>Pages need to be crawled regularly to ensure freshness</li>
<li>Average refresh rate of about once per week, more frequent for popular sites
<ul>
<li>4 billion links crawled each month</li>
</ul></li>
<li>Average stored size per web page: 500 KB
<ul>
<li>For simplicity, count changes the same as new pages</li>
</ul></li>
</ul></li>
<li>100 billion searches per month</li>
</ul>
<p>Exercise the use of more traditional systems - don’t use existing systems such as <a href="http://lucene.apache.org/solr/">solr</a> or <a href="http://nutch.apache.org/">nutch</a>.</p>
<h4 id="calculate-usage">Calculate usage</h4>
<p><strong>Clarify with your interviewer if you should run back-of-the-envelope usage calculations.</strong></p>
<ul>
<li>2 PB of stored page content per month
<ul>
<li>500 KB per page * 4 billion links crawled per month</li>
<li>72 PB of stored page content in 3 years</li>
</ul></li>
<li>1,600 write requests per second</li>
<li>40,000 search requests per second</li>
</ul>
<p>Handy conversion guide:</p>
<ul>
<li>2.5 million seconds per month</li>
<li>1 request per second = 2.5 million requests per month</li>
<li>40 requests per second = 100 million requests per month</li>
<li>400 requests per second = 1 billion requests per month</li>
</ul>
<h2 id="step-2-create-a-high-level-design">Step 2: Create a high level design</h2>
<blockquote>
<p>Outline a high level design with all important components.</p>
</blockquote>
<figure>
<img src="http://i.imgur.com/xjdAAUv.png" alt="Imgur" /><figcaption>Imgur</figcaption>
</figure>
<h2 id="step-3-design-core-components">Step 3: Design core components</h2>
<blockquote>
<p>Dive into details for each core component.</p>
</blockquote>
<h3 id="use-case-service-crawls-a-list-of-urls">Use case: Service crawls a list of urls</h3>
<p>We’ll assume we have an initial list of <code>links_to_crawl</code> ranked initially based on overall site popularity. If this is not a reasonable assumption, we can seed the crawler with popular sites that link to outside content such as <a href="https://www.yahoo.com/">Yahoo</a>, <a href="http://www.dmoz.org/">DMOZ</a>, etc.</p>
<p>We’ll use a table <code>crawled_links</code> to store processed links and their page signatures.</p>
<p>We could store <code>links_to_crawl</code> and <code>crawled_links</code> in a key-value <strong>NoSQL Database</strong>. For the ranked links in <code>links_to_crawl</code>, we could use <a href="https://redis.io/">Redis</a> with sorted sets to maintain a ranking of page links. We should discuss the <a href="https://github.com/donnemartin/system-design-primer#sql-or-nosql">use cases and tradeoffs between choosing SQL or NoSQL</a>.</p>
<ul>
<li>The <strong>Crawler Service</strong> processes each page link by doing the following in a loop:
<ul>
<li>Takes the top ranked page link to crawl
<ul>
<li>Checks <code>crawled_links</code> in the <strong>NoSQL Database</strong> for an entry with a similar page signature
<ul>
<li>If we have a similar page, reduces the priority of the page link
<ul>
<li>This prevents us from getting into a cycle</li>
<li>Continue</li>
</ul></li>
<li>Else, crawls the link
<ul>
<li>Adds a job to the <strong>Reverse Index Service</strong> queue to generate a <a href="https://en.wikipedia.org/wiki/Search_engine_indexing">reverse index</a></li>
<li>Adds a job to the <strong>Document Service</strong> queue to generate a static title and snippet</li>
<li>Generates the page signature</li>
<li>Removes the link from <code>links_to_crawl</code> in the <strong>NoSQL Database</strong></li>
<li>Inserts the page link and signature to <code>crawled_links</code> in the <strong>NoSQL Database</strong></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><strong>Clarify with your interviewer how much code you are expected to write</strong>.</p>
<p><code>PagesDataStore</code> is an abstraction within the <strong>Crawler Service</strong> that uses the <strong>NoSQL Database</strong>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="kw">class</span> PagesDataStore(<span class="bu">object</span>):</a>
<a class="sourceLine" id="cb1-2" title="2"></a>
<a class="sourceLine" id="cb1-3" title="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, db)<span class="op">;</span></a>
<a class="sourceLine" id="cb1-4" title="4">        <span class="va">self</span>.db <span class="op">=</span> db</a>
<a class="sourceLine" id="cb1-5" title="5">        ...</a>
<a class="sourceLine" id="cb1-6" title="6"></a>
<a class="sourceLine" id="cb1-7" title="7">    <span class="kw">def</span> add_link_to_crawl(<span class="va">self</span>, url):</a>
<a class="sourceLine" id="cb1-8" title="8">        <span class="co">&quot;&quot;&quot;Add the given link to `links_to_crawl`.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-9" title="9">        ...</a>
<a class="sourceLine" id="cb1-10" title="10"></a>
<a class="sourceLine" id="cb1-11" title="11">    <span class="kw">def</span> remove_link_to_crawl(<span class="va">self</span>, url):</a>
<a class="sourceLine" id="cb1-12" title="12">        <span class="co">&quot;&quot;&quot;Remove the given link from `links_to_crawl`.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-13" title="13">        ...</a>
<a class="sourceLine" id="cb1-14" title="14"></a>
<a class="sourceLine" id="cb1-15" title="15">    <span class="kw">def</span> reduce_priority_link_to_crawl(<span class="va">self</span>, url)</a>
<a class="sourceLine" id="cb1-16" title="16">        <span class="co">&quot;&quot;&quot;Reduce the priority of a link in `links_to_crawl` to avoid cycles.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-17" title="17">        ...</a>
<a class="sourceLine" id="cb1-18" title="18"></a>
<a class="sourceLine" id="cb1-19" title="19">    <span class="kw">def</span> extract_max_priority_page(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb1-20" title="20">        <span class="co">&quot;&quot;&quot;Return the highest priority link in `links_to_crawl`.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-21" title="21">        ...</a>
<a class="sourceLine" id="cb1-22" title="22"></a>
<a class="sourceLine" id="cb1-23" title="23">    <span class="kw">def</span> insert_crawled_link(<span class="va">self</span>, url, signature):</a>
<a class="sourceLine" id="cb1-24" title="24">        <span class="co">&quot;&quot;&quot;Add the given link to `crawled_links`.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-25" title="25">        ...</a>
<a class="sourceLine" id="cb1-26" title="26"></a>
<a class="sourceLine" id="cb1-27" title="27">    <span class="kw">def</span> crawled_similar(<span class="va">self</span>, signature):</a>
<a class="sourceLine" id="cb1-28" title="28">        <span class="co">&quot;&quot;&quot;Determine if we&#39;ve already crawled a page matching the given signature&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb1-29" title="29">        ...</a></code></pre></div>
<p><code>Page</code> is an abstraction within the <strong>Crawler Service</strong> that encapsulates a page, its contents, child urls, and signature:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="kw">class</span> Page(<span class="bu">object</span>):</a>
<a class="sourceLine" id="cb2-2" title="2"></a>
<a class="sourceLine" id="cb2-3" title="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, url, contents, child_urls, signature):</a>
<a class="sourceLine" id="cb2-4" title="4">        <span class="va">self</span>.url <span class="op">=</span> url</a>
<a class="sourceLine" id="cb2-5" title="5">        <span class="va">self</span>.contents <span class="op">=</span> contents</a>
<a class="sourceLine" id="cb2-6" title="6">        <span class="va">self</span>.child_urls <span class="op">=</span> child_urls</a>
<a class="sourceLine" id="cb2-7" title="7">        <span class="va">self</span>.signature <span class="op">=</span> signature</a></code></pre></div>
<p><code>Crawler</code> is the main class within <strong>Crawler Service</strong>, composed of <code>Page</code> and <code>PagesDataStore</code>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1"><span class="kw">class</span> Crawler(<span class="bu">object</span>):</a>
<a class="sourceLine" id="cb3-2" title="2"></a>
<a class="sourceLine" id="cb3-3" title="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data_store, reverse_index_queue, doc_index_queue):</a>
<a class="sourceLine" id="cb3-4" title="4">        <span class="va">self</span>.data_store <span class="op">=</span> data_store</a>
<a class="sourceLine" id="cb3-5" title="5">        <span class="va">self</span>.reverse_index_queue <span class="op">=</span> reverse_index_queue</a>
<a class="sourceLine" id="cb3-6" title="6">        <span class="va">self</span>.doc_index_queue <span class="op">=</span> doc_index_queue</a>
<a class="sourceLine" id="cb3-7" title="7"></a>
<a class="sourceLine" id="cb3-8" title="8">    <span class="kw">def</span> create_signature(<span class="va">self</span>, page):</a>
<a class="sourceLine" id="cb3-9" title="9">        <span class="co">&quot;&quot;&quot;Create signature based on url and contents.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb3-10" title="10">        ...</a>
<a class="sourceLine" id="cb3-11" title="11"></a>
<a class="sourceLine" id="cb3-12" title="12">    <span class="kw">def</span> crawl_page(<span class="va">self</span>, page):</a>
<a class="sourceLine" id="cb3-13" title="13">        <span class="cf">for</span> url <span class="kw">in</span> page.child_urls:</a>
<a class="sourceLine" id="cb3-14" title="14">            <span class="va">self</span>.data_store.add_link_to_crawl(url)</a>
<a class="sourceLine" id="cb3-15" title="15">        page.signature <span class="op">=</span> <span class="va">self</span>.create_signature(page)</a>
<a class="sourceLine" id="cb3-16" title="16">        <span class="va">self</span>.data_store.remove_link_to_crawl(page.url)</a>
<a class="sourceLine" id="cb3-17" title="17">        <span class="va">self</span>.data_store.insert_crawled_link(page.url, page.signature)</a>
<a class="sourceLine" id="cb3-18" title="18"></a>
<a class="sourceLine" id="cb3-19" title="19">    <span class="kw">def</span> crawl(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb3-20" title="20">        <span class="cf">while</span> <span class="va">True</span>:</a>
<a class="sourceLine" id="cb3-21" title="21">            page <span class="op">=</span> <span class="va">self</span>.data_store.extract_max_priority_page()</a>
<a class="sourceLine" id="cb3-22" title="22">            <span class="cf">if</span> page <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb3-23" title="23">                <span class="cf">break</span></a>
<a class="sourceLine" id="cb3-24" title="24">            <span class="cf">if</span> <span class="va">self</span>.data_store.crawled_similar(page.signature):</a>
<a class="sourceLine" id="cb3-25" title="25">                <span class="va">self</span>.data_store.reduce_priority_link_to_crawl(page.url)</a>
<a class="sourceLine" id="cb3-26" title="26">            <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb3-27" title="27">                <span class="va">self</span>.crawl_page(page)</a></code></pre></div>
<h3 id="handling-duplicates">Handling duplicates</h3>
<p>We need to be careful the web crawler doesn’t get stuck in an infinite loop, which happens when the graph contains a cycle.</p>
<p><strong>Clarify with your interviewer how much code you are expected to write</strong>.</p>
<p>We’ll want to remove duplicate urls:</p>
<ul>
<li>For smaller lists we could use something like <code>sort | unique</code></li>
<li>With 1 billion links to crawl, we could use <strong>MapReduce</strong> to output only entries that have a frequency of 1</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1"><span class="kw">class</span> RemoveDuplicateUrls(MRJob):</a>
<a class="sourceLine" id="cb4-2" title="2"></a>
<a class="sourceLine" id="cb4-3" title="3">    <span class="kw">def</span> mapper(<span class="va">self</span>, _, line):</a>
<a class="sourceLine" id="cb4-4" title="4">        <span class="cf">yield</span> line, <span class="dv">1</span></a>
<a class="sourceLine" id="cb4-5" title="5"></a>
<a class="sourceLine" id="cb4-6" title="6">    <span class="kw">def</span> reducer(<span class="va">self</span>, key, values):</a>
<a class="sourceLine" id="cb4-7" title="7">        total <span class="op">=</span> <span class="bu">sum</span>(values)</a>
<a class="sourceLine" id="cb4-8" title="8">        <span class="cf">if</span> total <span class="op">==</span> <span class="dv">1</span>:</a>
<a class="sourceLine" id="cb4-9" title="9">            <span class="cf">yield</span> key, total</a></code></pre></div>
<p>Detecting duplicate content is more complex. We could generate a signature based on the contents of the page and compare those two signatures for similarity. Some potential algorithms are <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard index</a> and <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>.</p>
<h3 id="determining-when-to-update-the-crawl-results">Determining when to update the crawl results</h3>
<p>Pages need to be crawled regularly to ensure freshness. Crawl results could have a <code>timestamp</code> field that indicates the last time a page was crawled. After a default time period, say one week, all pages should be refreshed. Frequently updated or more popular sites could be refreshed in shorter intervals.</p>
<p>Although we won’t dive into details on analytics, we could do some data mining to determine the mean time before a particular page is updated, and use that statistic to determine how often to re-crawl the page.</p>
<p>We might also choose to support a <code>Robots.txt</code> file that gives webmasters control of crawl frequency.</p>
<h3 id="use-case-user-inputs-a-search-term-and-sees-a-list-of-relevant-pages-with-titles-and-snippets">Use case: User inputs a search term and sees a list of relevant pages with titles and snippets</h3>
<ul>
<li>The <strong>Client</strong> sends a request to the <strong>Web Server</strong>, running as a <a href="https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server">reverse proxy</a></li>
<li>The <strong>Web Server</strong> forwards the request to the <strong>Query API</strong> server</li>
<li>The <strong>Query API</strong> server does the following:
<ul>
<li>Parses the query
<ul>
<li>Removes markup</li>
<li>Breaks up the text into terms</li>
<li>Fixes typos</li>
<li>Normalizes capitalization</li>
<li>Converts the query to use boolean operations</li>
</ul></li>
<li>Uses the <strong>Reverse Index Service</strong> to find documents matching the query
<ul>
<li>The <strong>Reverse Index Service</strong> ranks the matching results and returns the top ones</li>
</ul></li>
<li>Uses the <strong>Document Service</strong> to return titles and snippets</li>
</ul></li>
</ul>
<p>We’ll use a public <a href="https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest"><strong>REST API</strong></a>:</p>
<pre><code>$ curl https://search.com/api/v1/search?query=hello+world</code></pre>
<p>Response:</p>
<pre><code>{
    &quot;title&quot;: &quot;foo&#39;s title&quot;,
    &quot;snippet&quot;: &quot;foo&#39;s snippet&quot;,
    &quot;link&quot;: &quot;https://foo.com&quot;,
},
{
    &quot;title&quot;: &quot;bar&#39;s title&quot;,
    &quot;snippet&quot;: &quot;bar&#39;s snippet&quot;,
    &quot;link&quot;: &quot;https://bar.com&quot;,
},
{
    &quot;title&quot;: &quot;baz&#39;s title&quot;,
    &quot;snippet&quot;: &quot;baz&#39;s snippet&quot;,
    &quot;link&quot;: &quot;https://baz.com&quot;,
},</code></pre>
<p>For internal communications, we could use <a href="https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc">Remote Procedure Calls</a>.</p>
<h2 id="step-4-scale-the-design">Step 4: Scale the design</h2>
<blockquote>
<p>Identify and address bottlenecks, given the constraints.</p>
</blockquote>
<figure>
<img src="http://i.imgur.com/bWxPtQA.png" alt="Imgur" /><figcaption>Imgur</figcaption>
</figure>
<p><strong>Important: Do not simply jump right into the final design from the initial design!</strong></p>
<p>State you would 1) <strong>Benchmark/Load Test</strong>, 2) <strong>Profile</strong> for bottlenecks 3) address bottlenecks while evaluating alternatives and trade-offs, and 4) repeat. See <a href="../scaling_aws/README.md">Design a system that scales to millions of users on AWS</a> as a sample on how to iteratively scale the initial design.</p>
<p>It’s important to discuss what bottlenecks you might encounter with the initial design and how you might address each of them. For example, what issues are addressed by adding a <strong>Load Balancer</strong> with multiple <strong>Web Servers</strong>? <strong>CDN</strong>? <strong>Master-Slave Replicas</strong>? What are the alternatives and <strong>Trade-Offs</strong> for each?</p>
<p>We’ll introduce some components to complete the design and to address scalability issues. Internal load balancers are not shown to reduce clutter.</p>
<p><em>To avoid repeating discussions</em>, refer to the following <a href="https://github.com/donnemartin/system-design-primer#index-of-system-design-topics">system design topics</a> for main talking points, tradeoffs, and alternatives:</p>
<ul>
<li><a href="https://github.com/donnemartin/system-design-primer#domain-name-system">DNS</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#load-balancer">Load balancer</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#horizontal-scaling">Horizontal scaling</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server">Web server (reverse proxy)</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#application-layer">API server (application layer)</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#cache">Cache</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#nosql">NoSQL</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#consistency-patterns">Consistency patterns</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#availability-patterns">Availability patterns</a></li>
</ul>
<p>Some searches are very popular, while others are only executed once. Popular queries can be served from a <strong>Memory Cache</strong> such as Redis or Memcached to reduce response times and to avoid overloading the <strong>Reverse Index Service</strong> and <strong>Document Service</strong>. The <strong>Memory Cache</strong> is also useful for handling the unevenly distributed traffic and traffic spikes. Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.<sup><a href=https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know>1</a></sup></p>
<p>Below are a few other optimizations to the <strong>Crawling Service</strong>:</p>
<ul>
<li>To handle the data size and request load, the <strong>Reverse Index Service</strong> and <strong>Document Service</strong> will likely need to make heavy use sharding and federation.</li>
<li>DNS lookup can be a bottleneck, the <strong>Crawler Service</strong> can keep its own DNS lookup that is refreshed periodically</li>
<li>The <strong>Crawler Service</strong> can improve performance and reduce memory usage by keeping many open connections at a time, referred to as <a href="https://en.wikipedia.org/wiki/Connection_pool">connection pooling</a>
<ul>
<li>Switching to <a href="https://github.com/donnemartin/system-design-primer#user-datagram-protocol-udp">UDP</a> could also boost performance</li>
</ul></li>
<li>Web crawling is bandwidth intensive, ensure there is enough bandwidth to sustain high throughput</li>
</ul>
<h2 id="additional-talking-points">Additional talking points</h2>
<blockquote>
<p>Additional topics to dive into, depending on the problem scope and time remaining.</p>
</blockquote>
<h3 id="sql-scaling-patterns">SQL scaling patterns</h3>
<ul>
<li><a href="https://github.com/donnemartin/system-design-primer#master-slave-replication">Read replicas</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#federation">Federation</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#sharding">Sharding</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#denormalization">Denormalization</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#sql-tuning">SQL Tuning</a></li>
</ul>
<h4 id="nosql">NoSQL</h4>
<ul>
<li><a href="https://github.com/donnemartin/system-design-primer#key-value-store">Key-value store</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#document-store">Document store</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#wide-column-store">Wide column store</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#graph-database">Graph database</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#sql-or-nosql">SQL vs NoSQL</a></li>
</ul>
<h3 id="caching">Caching</h3>
<ul>
<li>Where to cache
<ul>
<li><a href="https://github.com/donnemartin/system-design-primer#client-caching">Client caching</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#cdn-caching">CDN caching</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#web-server-caching">Web server caching</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#database-caching">Database caching</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#application-caching">Application caching</a></li>
</ul></li>
<li>What to cache
<ul>
<li><a href="https://github.com/donnemartin/system-design-primer#caching-at-the-database-query-level">Caching at the database query level</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#caching-at-the-object-level">Caching at the object level</a></li>
</ul></li>
<li>When to update the cache
<ul>
<li><a href="https://github.com/donnemartin/system-design-primer#cache-aside">Cache-aside</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#write-through">Write-through</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#write-behind-write-back">Write-behind (write-back)</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#refresh-ahead">Refresh ahead</a></li>
</ul></li>
</ul>
<h3 id="asynchronism-and-microservices">Asynchronism and microservices</h3>
<ul>
<li><a href="https://github.com/donnemartin/system-design-primer#message-queues">Message queues</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#task-queues">Task queues</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#back-pressure">Back pressure</a></li>
<li><a href="https://github.com/donnemartin/system-design-primer#microservices">Microservices</a></li>
</ul>
<h3 id="communications">Communications</h3>
<ul>
<li>Discuss tradeoffs:
<ul>
<li>External communication with clients - <a href="https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest">HTTP APIs following REST</a></li>
<li>Internal communications - <a href="https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc">RPC</a></li>
</ul></li>
<li><a href="https://github.com/donnemartin/system-design-primer#service-discovery">Service discovery</a></li>
</ul>
<h3 id="security">Security</h3>
<p>Refer to the <a href="https://github.com/donnemartin/system-design-primer#security">security section</a>.</p>
<h3 id="latency-numbers">Latency numbers</h3>
<p>See <a href="https://github.com/donnemartin/system-design-primer#latency-numbers-every-programmer-should-know">Latency numbers every programmer should know</a>.</p>
<h3 id="ongoing">Ongoing</h3>
<ul>
<li>Continue benchmarking and monitoring your system to address bottlenecks as they come up</li>
<li>Scaling is an iterative process</li>
</ul>
</body>
</html>
